## Introduction
A curated list of awesome papers, codes and blogs about Diffusion Large Language Models. 

Amazing links to try DiffusionLLM:
- [Mercury](https://www.inceptionlabs.ai/introducing-mercury)
- [Gemini-diffusion](https://deepmind.google/models/gemini-diffusion/)

Some related repo:
- [discrete-diffusion-papers](https://github.com/hanyang1999/discrete-diffusion-papers)
- [Diffusion-LLM-Papers](https://github.com/ML-GSAI/Diffusion-LLM-Papers)

## Contents
* [Preliminary](#preliminary)
* [Models](#models)
* [RL + DLLM](#rl--dllm)
* [Efficiency](#efficiency)


### Preliminary
<div id="preliminary"></div>

|Date|Title|Paper|Code|
|:---:|:---:|:---:|:---:|
|2021.07|Structured Denoising Diffusion Models in Discrete State-Spaces (@Google), NeurIPS 2021| [pdf](https://arxiv.org/pdf/2107.03006) | / |
|2021.11|Improved Denoising Diffusion Probabilistic Models (@OpenAI), NeurIPS 2021| [pdf](https://arxiv.org/pdf/2102.09672)| [improved-diffusion](https://github.com/openai/improved-diffusion) |
|2024.06|Simplified and Generalized Masked Diffusion for Discrete Data (@Google), NeurIPS 2024|[pdf](https://arxiv.org/pdf/2406.04329)|[MD4](https://github.com/google-deepmind/md4)|
|2024.06|Simple and Effective Masked Diffusion Language Models (@Cornell Tech), NeurIPS 2024|[pdf](https://arxiv.org/pdf/2406.07524)|[mdlm](https://github.com/kuleshov-group/mdlm)|
|2024.06|Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data (@RUC), ICLR 2025|[pdf](https://arxiv.org/pdf/2406.03736)|[RADD](https://github.com/ML-GSAI/RADD)|
|2024.09|Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling (@THU & NVIDIA), ICLR 2025|[pdf](https://arxiv.org/pdf/2409.02908)|/|

### Models
<div id="models"></div>

|Date|Title|Paper|Code|
|:---:|:---:|:---:|:---:|
|2025.02| Large Language Diffusion Models (@RUC) | [pdf](https://arxiv.org/pdf/2502.09992) | [LLaDA](https://github.com/ML-GSAI/LLaDA)|
|2025.03| Dream 7B (@HKU) | [blog](https://hkunlp.github.io/blog/2025/dream/) | [Dream](https://github.com/HKUNLP/Dream) |
|2025.05| MMaDA: Multimodal Large Diffusion Language Models (@Princeton,PKU,THU & ByteDance) | [pdf](https://arxiv.org/pdf/2505.15809) | [MMaDA](https://github.com/Gen-Verse/MMaDA)|
|2025.05| LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning|[pdf](https://arxiv.org/pdf/2505.16933)|[LLaDA-V](https://github.com/ML-GSAI/LLaDA-V)|
| 2025.05 | LaViDa: A Large Diffusion Language Model for Multimodal Understanding (@UCLA,Panasonic,Adobe,Salesforce) |  [pdf](http://arxiv.org/pdf/2505.19223) | [LaViDa](https://github.com/jacklishufan/LaViDa) |
| 2025.05 | Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel Decoding(@NUS)|   [pdf](https://arxiv.org/pdf/2505.16990) | [Dimple](https://github.com/yu-rp/Dimple) |

### RL + DLLM
<div id="rl--dllm"></div>

|Date|Title|Paper|Code|
|:---:|:---:|:---:|:---:|
| 2025.04 | d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning (@UCLA,Meta) | [pdf](https://arxiv.org/pdf/2504.12216) | [d1](https://github.com/dllm-reasoning/d1) |
| 2025.05 | LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models (@RUC,Beijing-AISI) | [pdf](http://arxiv.org/pdf/2505.19223) | [LLaDA-1.5](https://github.com/ML-GSAI/LLaDA-1.5) |
| 2025.05 | Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models (@UCLA,Meta) | [pdf](https://arxiv.org/pdf/2505.10446) | / |

### Efficiency
<div id="efficiency"></div>

|Date|Title|Paper|Code|
|:---:|:---:|:---:|:---:|
|2025.03|Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models (@Cornell,Stanford & Cohere), ICLR 2025 oral| [pdf](https://arxiv.org/pdf/2503.09573)|[bd3lms](https://github.com/kuleshov-group/bd3lms)|
|2025.05|Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion (@Cornell)|[pdf](https://arxiv.org/pdf/2505.21467) | / |
|2025.05|Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding (@NVIDIA) |[pdf](https://arxiv.org/pdf/2505.22618)|[Fast-dLLM](https://github.com/NVlabs/Fast-dLLM)
|2025.05|dKV-Cache: The Cache for Diffusion Language Models(@NUS)|[pdf](https://arxiv.org/pdf/2505.15781) | [dKV-Cache](https://github.com/horseee/dKV-Cache) |
|2025.05|Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking(@Meta)|[pdf](https://arxiv.org/pdf/2505.24857) | / |
|2025.05|Accelerating Diffusion LLMs via Adaptive Parallel Decoding (@UCLA)|[pdf](https://www.arxiv.org/pdf/2506.00413)| / |
|2025.05|Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel Decoding(@NUS) |[pdf](https://arxiv.org/pdf/2505.16990)| [Dimple](https://github.com/yu-rp/Dimple)|
|2025.06|Esoteric Language Models(@Cornell) |[pdf](https://arxiv.org/pdf/2506.01928)|[Eso-LMs](https://s-sahoo.com/Eso-LMs/)|
