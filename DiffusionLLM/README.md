## Introduction
A curated list of awesome papers, codes and blogs about Diffusion Large Language Models. 

Amazing links to try DiffusionLLM:
- [Mercury](https://www.inceptionlabs.ai/introducing-mercury)
- [Gemini-diffusion](https://deepmind.google/models/gemini-diffusion/)

Some related repo:
- [discrete-diffusion-papers](https://github.com/hanyang1999/discrete-diffusion-papers)
- [Diffusion-LLM-Papers](https://github.com/ML-GSAI/Diffusion-LLM-Papers)

## Contents
* [Preliminary](#preliminary)
* [Models](#models)
* [Efficiency](#efficiency)


### Preliminary
<div id="preliminary"></div>

|Date|Title|Paper|Code|
|:---:|:---:|:---:|:---:|
|2021.07|Structured Denoising Diffusion Models in Discrete State-Spaces (@Google), NeurIPS 2021| [pdf](https://arxiv.org/pdf/2107.03006) | / |
|2021.11|Improved Denoising Diffusion Probabilistic Models (@OpenAI), NeurIPS 2021| [pdf](https://arxiv.org/pdf/2102.09672)| [improved-diffusion](https://github.com/openai/improved-diffusion) |
|2024.06|Simplified and Generalized Masked Diffusion for Discrete Data (@Google), NeurIPS 2024|[pdf](https://arxiv.org/pdf/2406.04329)|[MD4](https://github.com/google-deepmind/md4)|
|2024.06|Simple and Effective Masked Diffusion Language Models (@Cornell Tech), NeurIPS 2024|[pdf](https://arxiv.org/pdf/2406.07524)|[mdlm](https://github.com/kuleshov-group/mdlm)|
|2024.06|Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data (@RUC), ICLR 2025|[pdf](https://arxiv.org/pdf/2406.03736)|[RADD](https://github.com/ML-GSAI/RADD)|
|2024.09|Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling (@THU & NVIDIA), ICLR 2025|[pdf](https://arxiv.org/pdf/2409.02908)|/|

### Models
<div id="models"></div>

|Date|Title|Paper|Code|
|:---:|:---:|:---:|:---:|
|2025.02|Large Language Diffusion Models (@RUC)|[pdf](https://arxiv.org/pdf/2502.09992)|[LLaDA](https://github.com/ML-GSAI/LLaDA)|
|2025.03|Dream 7B (@HKU)|[blog](https://hkunlp.github.io/blog/2025/dream/)|[Dream](https://github.com/HKUNLP/Dream)|
|2025.05|MMaDA: Multimodal Large Diffusion Language Models (@Princeton,PKU,THU & ByteDance)|[pdf](https://arxiv.org/pdf/2505.15809)|[MMaDA](https://github.com/Gen-Verse/MMaDA)|
|2025.05|LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning|[pdf](https://arxiv.org/pdf/2505.16933)|[LLaDA-V](https://github.com/ML-GSAI/LLaDA-V)|
|2025.05| LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models |      [pdf](http://arxiv.org/pdf/2505.19223)       | [LLaDA-1.5](https://github.com/ML-GSAI/LLaDA-1.5) |

### Efficiency
<div id="efficiency"></div>

|Date|Title|Paper|Code|
|:---:|:---:|:---:|:---:|
|2025.03|Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models (@Cornell,Stanford & Cohere), ICLR 2025 oral| [pdf](https://arxiv.org/pdf/2503.09573)|[bd3lms](https://github.com/kuleshov-group/bd3lms)|
|2025.05|Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion (@Cornell)|[pdf](https://arxiv.org/pdf/2505.21467) | / |
|2025.05|Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding (@NVIDIA) |[pdf](https://arxiv.org/pdf/2505.22618)|[Fast-dLLM](https://github.com/NVlabs/Fast-dLLM)
